{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balnarendrasapa/road-detection/blob/master/submissions/final-submission/final_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the Repository\n",
        "\n",
        "[Repository Link](https://github.com/balnarendrasapa/road-detection)\n",
        "\n",
        "- This is our team's repository. This repository contains all the necessary code that we worked on and it also contains the dataset that we annotated.\n",
        "\n",
        "- You do not need to do anything like uploading and adjusting the paths. Just run the cells sequentially.\n",
        "\n",
        "- All the necessary commands are written in this notebook itself"
      ],
      "metadata": {
        "id": "JzycIPSy2AKH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyznWPpKmNIs",
        "outputId": "6a650029-6cad-469a-d424-3c9ef5ec0adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'road-detection'...\n",
            "remote: Enumerating objects: 627, done.\u001b[K\n",
            "remote: Counting objects: 100% (368/368), done.\u001b[K\n",
            "remote: Compressing objects: 100% (255/255), done.\u001b[K\n",
            "remote: Total 627 (delta 135), reused 227 (delta 86), pack-reused 259\u001b[K\n",
            "Receiving objects: 100% (627/627), 212.69 MiB | 15.14 MiB/s, done.\n",
            "Resolving deltas: 100% (228/228), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/balnarendrasapa/road-detection.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the Requirements\n",
        "\n",
        "- Install all the python dependencies\n",
        "- After Installing dependencies, Restart the runtime. If you do not restart the runtime, the python will throw \"module not found error\""
      ],
      "metadata": {
        "id": "AVXcandz2wFA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "298SpxZcDf1R",
        "outputId": "7bb48d71-8e21-47b7-ba6e-42bab3bb8752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting certifi==2023.7.22 (from -r road-detection/TwinLiteNet/requirements.txt (line 1))\n",
            "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m906.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer==3.3.2 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 2)) (3.3.2)\n",
            "Collecting colorama==0.4.6 (from -r road-detection/TwinLiteNet/requirements.txt (line 3))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: contourpy==1.2.0 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 5)) (0.12.1)\n",
            "Collecting dnspython==2.4.2 (from -r road-detection/TwinLiteNet/requirements.txt (line 6))\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting elephant==0.12.0 (from -r road-detection/TwinLiteNet/requirements.txt (line 7))\n",
            "  Downloading elephant-0.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock==3.13.1 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 8)) (3.13.1)\n",
            "Collecting fonttools==4.44.0 (from -r road-detection/TwinLiteNet/requirements.txt (line 9))\n",
            "  Downloading fonttools-4.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec==2023.10.0 (from -r road-detection/TwinLiteNet/requirements.txt (line 10))\n",
            "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==3.4 (from -r road-detection/TwinLiteNet/requirements.txt (line 11))\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2==3.1.2 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 12)) (3.1.2)\n",
            "Collecting joblib==1.2.0 (from -r road-detection/TwinLiteNet/requirements.txt (line 13))\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver==1.4.5 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 14)) (1.4.5)\n",
            "Requirement already satisfied: MarkupSafe==2.1.3 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 15)) (2.1.3)\n",
            "Requirement already satisfied: matplotlib==3.7.1 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 16)) (3.7.1)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 17)) (1.3.0)\n",
            "Collecting neo==0.12.0 (from -r road-detection/TwinLiteNet/requirements.txt (line 18))\n",
            "  Downloading neo-0.12.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx==3.2.1 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 19)) (3.2.1)\n",
            "Collecting numpy==1.24.3 (from -r road-detection/TwinLiteNet/requirements.txt (line 20))\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python==4.7.0.72 (from -r road-detection/TwinLiteNet/requirements.txt (line 21))\n",
            "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging==23.2 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 22)) (23.2)\n",
            "Collecting Pillow==9.5.0 (from -r road-detection/TwinLiteNet/requirements.txt (line 23))\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing==3.1.1 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 24)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 25)) (2.8.2)\n",
            "Collecting python-etcd==0.4.5 (from -r road-detection/TwinLiteNet/requirements.txt (line 26))\n",
            "  Downloading python-etcd-0.4.5.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 27)) (6.0.1)\n",
            "Collecting quantities==0.14.1 (from -r road-detection/TwinLiteNet/requirements.txt (line 28))\n",
            "  Downloading quantities-0.14.1-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 29)) (2.31.0)\n",
            "Collecting scikit-learn==1.3.2 (from -r road-detection/TwinLiteNet/requirements.txt (line 30))\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.10.1 (from -r road-detection/TwinLiteNet/requirements.txt (line 31))\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 32)) (1.16.0)\n",
            "Requirement already satisfied: sympy==1.12 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 33)) (1.12)\n",
            "Requirement already satisfied: threadpoolctl==3.2.0 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 34)) (3.2.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 35)) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchdata==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 36)) (0.7.0)\n",
            "Collecting torchelastic==0.2.2 (from -r road-detection/TwinLiteNet/requirements.txt (line 37))\n",
            "  Downloading torchelastic-0.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.5/111.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchtext==0.16.0 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 38)) (0.16.0)\n",
            "Requirement already satisfied: torchvision==0.16.0 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 39)) (0.16.0+cu121)\n",
            "Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 40)) (4.66.1)\n",
            "Collecting typing_extensions==4.8.0 (from -r road-detection/TwinLiteNet/requirements.txt (line 41))\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: urllib3==2.0.7 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 42)) (2.0.7)\n",
            "Requirement already satisfied: webcolors==1.13 in /usr/local/lib/python3.10/dist-packages (from -r road-detection/TwinLiteNet/requirements.txt (line 43)) (1.13)\n",
            "Collecting yacs==0.1.8 (from -r road-detection/TwinLiteNet/requirements.txt (line 44))\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Collecting zipp==3.15.0 (from -r road-detection/TwinLiteNet/requirements.txt (line 45))\n",
            "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r road-detection/TwinLiteNet/requirements.txt (line 35)) (2.1.0)\n",
            "Building wheels for collected packages: python-etcd\n",
            "  Building wheel for python-etcd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-etcd: filename=python_etcd-0.4.5-py3-none-any.whl size=38481 sha256=600ae4ea8608bc3c5d52d07eff40e9f1316b8e2d793a0623af439aaca7941e24\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/5f/1b/056db07a0ab1c0b7efe175928d2a10b614e0e00d7bab0b6496\n",
            "Successfully built python-etcd\n",
            "Installing collected packages: zipp, yacs, typing_extensions, Pillow, numpy, joblib, idna, fsspec, fonttools, dnspython, colorama, certifi, scipy, quantities, python-etcd, opencv-python, torchelastic, scikit-learn, neo, elephant\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.17.0\n",
            "    Uninstalling zipp-3.17.0:\n",
            "      Successfully uninstalled zipp-3.17.0\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.3.2\n",
            "    Uninstalling joblib-1.3.2:\n",
            "      Successfully uninstalled joblib-1.3.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.6.0\n",
            "    Uninstalling fsspec-2023.6.0:\n",
            "      Successfully uninstalled fsspec-2023.6.0\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.46.0\n",
            "    Uninstalling fonttools-4.46.0:\n",
            "      Successfully uninstalled fonttools-4.46.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.11.17\n",
            "    Uninstalling certifi-2023.11.17:\n",
            "      Successfully uninstalled certifi-2023.11.17\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.8.0.76\n",
            "    Uninstalling opencv-python-4.8.0.76:\n",
            "      Successfully uninstalled opencv-python-4.8.0.76\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-9.5.0 certifi-2023.7.22 colorama-0.4.6 dnspython-2.4.2 elephant-0.12.0 fonttools-4.44.0 fsspec-2023.10.0 idna-3.4 joblib-1.2.0 neo-0.12.0 numpy-1.24.3 opencv-python-4.7.0.72 python-etcd-0.4.5 quantities-0.14.1 scikit-learn-1.3.2 scipy-1.10.1 torchelastic-0.2.2 typing_extensions-4.8.0 yacs-0.1.8 zipp-3.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r road-detection/TwinLiteNet/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copy Dataset from Repository\n",
        "\n",
        "- Our repository contains dataset.zip in datasets folder in the repository. copy that zip file to root"
      ],
      "metadata": {
        "id": "WtYxavR2503Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp road-detection/datasets/dataset.zip ./"
      ],
      "metadata": {
        "id": "ihjXltFR1OQI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzip the file"
      ],
      "metadata": {
        "id": "ferlFJ_76GBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset.zip"
      ],
      "metadata": {
        "id": "w7AUZJZ0f491"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the all the required libraries"
      ],
      "metadata": {
        "id": "bpUdANiK6K-i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hVDJcpeP5d1J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image transformation functions\n",
        "\n",
        "- By paper author"
      ],
      "metadata": {
        "id": "MXX5-aH58B4c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ywi8_wbg5jZQ"
      },
      "outputs": [],
      "source": [
        "def augment_hsv(img, hgain=0.015, sgain=0.7, vgain=0.4):\n",
        "    \"\"\"change color hue, saturation, value\"\"\"\n",
        "    r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains\n",
        "    hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\n",
        "    dtype = img.dtype  # uint8\n",
        "\n",
        "    x = np.arange(0, 256, dtype=np.int16)\n",
        "    lut_hue = ((x * r[0]) % 180).astype(dtype)\n",
        "    lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n",
        "    lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n",
        "\n",
        "    img_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val))).astype(dtype)\n",
        "    cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR, dst=img)  # no return needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NMEu5Ey35mWQ"
      },
      "outputs": [],
      "source": [
        "def random_perspective(combination,  degrees=10, translate=.1, scale=.1, shear=10, perspective=0.0, border=(0, 0)):\n",
        "    \"\"\"combination of img transform\"\"\"\n",
        "    # torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(.1, .1), scale=(.9, 1.1), shear=(-10, 10))\n",
        "    # targets = [cls, xyxy]\n",
        "    img, gray, line = combination\n",
        "    height = img.shape[0] + border[0] * 2  # shape(h,w,c)\n",
        "    width = img.shape[1] + border[1] * 2\n",
        "\n",
        "    # Center\n",
        "    C = np.eye(3)\n",
        "    C[0, 2] = -img.shape[1] / 2  # x translation (pixels)\n",
        "    C[1, 2] = -img.shape[0] / 2  # y translation (pixels)\n",
        "\n",
        "    # Perspective\n",
        "    P = np.eye(3)\n",
        "    P[2, 0] = random.uniform(-perspective, perspective)  # x perspective (about y)\n",
        "    P[2, 1] = random.uniform(-perspective, perspective)  # y perspective (about x)\n",
        "\n",
        "    # Rotation and Scale\n",
        "    R = np.eye(3)\n",
        "    a = random.uniform(-degrees, degrees)\n",
        "    # a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations\n",
        "    s = random.uniform(1 - scale, 1 + scale)\n",
        "    # s = 2 ** random.uniform(-scale, scale)\n",
        "    R[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n",
        "\n",
        "    # Shear\n",
        "    S = np.eye(3)\n",
        "    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # x shear (deg)\n",
        "    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # y shear (deg)\n",
        "\n",
        "    # Translation\n",
        "    T = np.eye(3)\n",
        "    T[0, 2] = random.uniform(0.5 - translate, 0.5 + translate) * width  # x translation (pixels)\n",
        "    T[1, 2] = random.uniform(0.5 - translate, 0.5 + translate) * height  # y translation (pixels)\n",
        "\n",
        "    # Combined rotation matrix\n",
        "    M = T @ S @ R @ P @ C  # order of operations (right to left) is IMPORTANT\n",
        "    if (border[0] != 0) or (border[1] != 0) or (M != np.eye(3)).any():  # image changed\n",
        "        if perspective:\n",
        "            img = cv2.warpPerspective(img, M, dsize=(width, height), borderValue=(114, 114, 114))\n",
        "            gray = cv2.warpPerspective(gray, M, dsize=(width, height), borderValue=0)\n",
        "            line = cv2.warpPerspective(line, M, dsize=(width, height), borderValue=0)\n",
        "        else:  # affine\n",
        "            img = cv2.warpAffine(img, M[:2], dsize=(width, height), borderValue=(114, 114, 114))\n",
        "            gray = cv2.warpAffine(gray, M[:2], dsize=(width, height), borderValue=0)\n",
        "            line = cv2.warpAffine(line, M[:2], dsize=(width, height), borderValue=0)\n",
        "\n",
        "\n",
        "\n",
        "    combination = (img, gray, line)\n",
        "    return combination"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir custom-dataset\n",
        "!mkdir custom-dataset/test\n",
        "!mkdir custom-dataset/train\n",
        "!mkdir custom-dataset/validation\n",
        "\n",
        "!mkdir -p custom-dataset/train/images\n",
        "!mkdir -p custom-dataset/train/lane\n",
        "!mkdir -p custom-dataset/train/segments\n",
        "!mkdir -p custom-dataset/test/images\n",
        "!mkdir -p custom-dataset/test/lane\n",
        "!mkdir -p custom-dataset/test/segments\n",
        "!mkdir -p custom-dataset/validation/images\n",
        "!mkdir -p custom-dataset/validation/lane\n",
        "!mkdir -p custom-dataset/validation/segments\n",
        "\n",
        "# Copy only two images from each category in train\n",
        "!cp dataset/train/images/road_image_0.png custom-dataset/train/images/\n",
        "!cp dataset/train/lane/road_image_0.png custom-dataset/train/lane/\n",
        "!cp dataset/train/segments/road_image_0.png custom-dataset/train/segments/\n",
        "\n",
        "# Copy only two images from each category in train\n",
        "!cp dataset/train/images/road_image_1.png custom-dataset/train/images/\n",
        "!cp dataset/train/lane/road_image_1.png custom-dataset/train/lane/\n",
        "!cp dataset/train/segments/road_image_1.png custom-dataset/train/segments/\n",
        "\n",
        "# Copy only two images from each category in test\n",
        "!cp dataset/test/images/* custom-dataset/test/images/\n",
        "!cp dataset/test/lane/* custom-dataset/test/lane/\n",
        "!cp dataset/test/segments/* custom-dataset/test/segments/\n",
        "\n",
        "\n",
        "# Copy only two images from each category in validation\n",
        "!cp dataset/validation/images/* custom-dataset/validation/images/\n",
        "!cp dataset/validation/lane/* custom-dataset/validation/lane/\n",
        "!cp dataset/validation/segments/* custom-dataset/validation/segments/"
      ],
      "metadata": {
        "id": "8XX_6sn33Qpc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Dataset Class\n",
        "\n",
        "- This custom dataset class is based on the dataset class written by the author but with slight modifications like path. we have adjusted the path according to the google colab."
      ],
      "metadata": {
        "id": "mFv9HU486TLr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_LoqglKDR2Sw"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    Class to load the dataset\n",
        "    '''\n",
        "    def __init__(self, transform=None, valid=False, test=False):\n",
        "        '''\n",
        "        :param imList: image list (Note that these lists have been processed and pickled using the loadData.py)\n",
        "        :param labelList: label list (Note that these lists have been processed and pickled using the loadData.py)\n",
        "        :param transform: Type of transformation. SEe Transforms.py for supported transformations\n",
        "        '''\n",
        "\n",
        "        self.transform = transform\n",
        "        self.Tensor = transforms.ToTensor()\n",
        "        self.valid=valid\n",
        "        if valid:\n",
        "            self.root='custom-dataset/validation/images'\n",
        "            self.names=os.listdir(self.root)\n",
        "        elif test:\n",
        "            self.root='custom-dataset/test/images'\n",
        "            self.names=os.listdir(self.root)\n",
        "        else:\n",
        "            self.root='custom-dataset/train/images/'\n",
        "            self.names=os.listdir(self.root)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "\n",
        "        :param idx: Index of the image file\n",
        "        :return: returns the image and corresponding label file.\n",
        "        '''\n",
        "        W_=640\n",
        "        H_=360\n",
        "        image_name=os.path.join(self.root,self.names[idx])\n",
        "\n",
        "        image = cv2.imread(image_name)\n",
        "        original_image = cv2.imread(image_name)\n",
        "        label1 = cv2.imread(image_name.replace(\"images\",\"segments\").replace(\"jpg\",\"png\"), 0)\n",
        "        label2 = cv2.imread(image_name.replace(\"images\",\"lane\").replace(\"jpg\",\"png\"), 0)\n",
        "        if not self.valid:\n",
        "            if random.random()<0.5:\n",
        "                combination = (image, label1, label2)\n",
        "                (image, label1, label2)= random_perspective(\n",
        "                    combination=combination,\n",
        "                    degrees=10,\n",
        "                    translate=0.1,\n",
        "                    scale=0.25,\n",
        "                    shear=0.0\n",
        "                )\n",
        "            if random.random()<0.5:\n",
        "                augment_hsv(image)\n",
        "            if random.random() < 0.5:\n",
        "                image = np.fliplr(image)\n",
        "                label1 = np.fliplr(label1)\n",
        "                label2 = np.fliplr(label2)\n",
        "\n",
        "        label1 = cv2.resize(label1, (W_, H_))\n",
        "        label2 = cv2.resize(label2, (W_, H_))\n",
        "        image = cv2.resize(image, (W_, H_))\n",
        "\n",
        "        _,seg_b1 = cv2.threshold(label1,1,255,cv2.THRESH_BINARY_INV)\n",
        "        _,seg_b2 = cv2.threshold(label2,1,255,cv2.THRESH_BINARY_INV)\n",
        "        _,seg1 = cv2.threshold(label1,1,255,cv2.THRESH_BINARY)\n",
        "        _,seg2 = cv2.threshold(label2,1,255,cv2.THRESH_BINARY)\n",
        "\n",
        "        seg1 = self.Tensor(seg1)\n",
        "        seg2 = self.Tensor(seg2)\n",
        "        seg_b1 = self.Tensor(seg_b1)\n",
        "        seg_b2 = self.Tensor(seg_b2)\n",
        "        seg_da = torch.stack((seg_b1[0], seg1[0]),0)\n",
        "        seg_ll = torch.stack((seg_b2[0], seg2[0]),0)\n",
        "        image = image[:, :, ::-1].transpose(2, 0, 1)\n",
        "        image = np.ascontiguousarray(image)\n",
        "\n",
        "        return original_image, image_name,torch.from_numpy(image),(seg_da,seg_ll)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intialize a dataloader\n",
        "\n",
        "- Intialize a dataloader with batch size 8\n",
        "\n",
        "- Intialize train, test, validation datasets."
      ],
      "metadata": {
        "id": "b6Ly9Ek16kg-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qIK3UcD3STAG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(MyDataset(), batch_size = 2, shuffle = True)\n",
        "test_dataloader = DataLoader(MyDataset(test=True), batch_size = 8, shuffle = True)\n",
        "val_dataloader = DataLoader(MyDataset(valid=True), batch_size = 8, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, _, input, target = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "cJ_g-ICbZUR1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy pretrained model from repository to root\n",
        "!cp road-detection/TwinLiteNet/pretrained/best.pth ./\n",
        "\n",
        "# Copy pytorch Neural Net from repo to root\n",
        "!cp road-detection/TwinLiteNet/model/TwinLite.py ./\n",
        "\n",
        "# Copy Loss function pytorch code from repo to root\n",
        "!cp road-detection/TwinLiteNet/loss.py ./\n",
        "\n",
        "# Copy all reqired constants from repo to root\n",
        "!cp road-detection/TwinLiteNet/const.py ./\n",
        "\n",
        "# Copy all val.py from repo to root\n",
        "!cp road-detection/TwinLiteNet/val.py ./"
      ],
      "metadata": {
        "id": "T5QDw0IqZgeR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MiniNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MiniNet, self).__init__()\n",
        "\n",
        "        # Input Convolution\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # Downsampling\n",
        "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Intermediate Convolutions\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        # Upsampling\n",
        "        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(16)\n",
        "        self.relu5 = nn.ReLU()\n",
        "\n",
        "        # Classifiers\n",
        "        self.classifier1 = nn.Conv2d(16, 2, kernel_size=1)\n",
        "        self.classifier2 = nn.Conv2d(16, 2, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input Convolution\n",
        "        x = self.relu1(self.bn1(self.conv1(x)))\n",
        "\n",
        "        # Downsampling\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Intermediate Convolutions\n",
        "        x = self.relu2(self.bn2(self.conv2(x)))\n",
        "        x = self.relu3(self.bn3(self.conv3(x)))\n",
        "\n",
        "        # Upsampling\n",
        "        x = self.relu4(self.bn4(self.upconv1(x)))\n",
        "        x = self.relu5(self.bn5(self.upconv2(x)))\n",
        "\n",
        "        # Classifiers\n",
        "        classifier1 = self.classifier1(x)\n",
        "        classifier2 = self.classifier2(x)\n",
        "\n",
        "        return classifier1, classifier2\n",
        "\n",
        "# Instantiate the model\n",
        "model = MiniNet().to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "3U3eUnw0ZqhX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from loss import TotalLoss"
      ],
      "metadata": {
        "id": "3uHGAKHHZk8v"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), 5e-4, (0.9, 0.999), eps=1e-08, weight_decay=5e-4)\n",
        "criterion = TotalLoss()"
      ],
      "metadata": {
        "id": "ac7Fw1ksZoBh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class SegmentationMetric(object):\n",
        "    '''\n",
        "    imgLabel [batch_size, height(144), width(256)]\n",
        "    confusionMatrix [[0(TN),1(FP)],\n",
        "                     [2(FN),3(TP)]]\n",
        "    '''\n",
        "    def __init__(self, numClass):\n",
        "        self.numClass = numClass\n",
        "        self.confusionMatrix = np.zeros((self.numClass,)*2)\n",
        "\n",
        "    def pixelAccuracy(self):\n",
        "        # return all class overall pixel accuracy\n",
        "        # acc = (TP + TN) / (TP + TN + FP + TN)\n",
        "        acc = np.diag(self.confusionMatrix).sum() /  self.confusionMatrix.sum()\n",
        "        return acc\n",
        "\n",
        "\n",
        "    def classPixelAccuracy(self):\n",
        "        # return each category pixel accuracy(A more accurate way to call it precision)\n",
        "        # acc = (TP) / TP + FP\n",
        "        classAcc = np.diag(self.confusionMatrix) / (self.confusionMatrix.sum(axis=0) + 1e-12)\n",
        "        return classAcc\n",
        "\n",
        "    def meanPixelAccuracy(self):\n",
        "        classAcc = self.classPixelAccuracy()\n",
        "        meanAcc = np.nanmean(classAcc)\n",
        "        return meanAcc\n",
        "\n",
        "    def meanIntersectionOverUnion(self):\n",
        "        # Intersection = TP Union = TP + FP + FN\n",
        "        # IoU = TP / (TP + FP + FN)\n",
        "        intersection = np.diag(self.confusionMatrix)\n",
        "        union = np.sum(self.confusionMatrix, axis=1) + np.sum(self.confusionMatrix, axis=0) - np.diag(self.confusionMatrix)\n",
        "        IoU = intersection / union\n",
        "        IoU[np.isnan(IoU)] = 0\n",
        "        mIoU = np.nanmean(IoU)\n",
        "        return mIoU\n",
        "\n",
        "    def IntersectionOverUnion(self):\n",
        "        intersection = np.diag(self.confusionMatrix)\n",
        "        union = np.sum(self.confusionMatrix, axis=1) + np.sum(self.confusionMatrix, axis=0) - np.diag(self.confusionMatrix)\n",
        "        IoU = intersection / union\n",
        "        IoU[np.isnan(IoU)] = 0\n",
        "        return IoU[1]\n",
        "\n",
        "    def genConfusionMatrix(self, imgPredict, imgLabel):\n",
        "        # remove classes from unlabeled pixels in gt image and predict\n",
        "        # print(imgLabel.shape)\n",
        "        mask = (imgLabel >= 0) & (imgLabel < self.numClass)\n",
        "        label = self.numClass * imgLabel[mask] + imgPredict[mask]\n",
        "        count = np.bincount(label, minlength=self.numClass**2)\n",
        "        confusionMatrix = count.reshape(self.numClass, self.numClass)\n",
        "        return confusionMatrix\n",
        "\n",
        "    def Frequency_Weighted_Intersection_over_Union(self):\n",
        "        # FWIOU =     [(TP+FN)/(TP+FP+TN+FN)] *[TP / (TP + FP + FN)]\n",
        "        freq = np.sum(self.confusionMatrix, axis=1) / np.sum(self.confusionMatrix)\n",
        "        iu = np.diag(self.confusionMatrix) / (\n",
        "                np.sum(self.confusionMatrix, axis=1) + np.sum(self.confusionMatrix, axis=0) -\n",
        "                np.diag(self.confusionMatrix))\n",
        "        FWIoU = (freq[freq > 0] * iu[freq > 0]).sum()\n",
        "        return FWIoU\n",
        "\n",
        "\n",
        "    def addBatch(self, imgPredict, imgLabel):\n",
        "        assert imgPredict.shape == imgLabel.shape\n",
        "        self.confusionMatrix += self.genConfusionMatrix(imgPredict, imgLabel)\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusionMatrix = np.zeros((self.numClass, self.numClass))\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count if self.count != 0 else 0\n",
        "\n",
        "@torch.no_grad()\n",
        "def val(val_loader, model):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    DA=SegmentationMetric(2)\n",
        "    LL=SegmentationMetric(2)\n",
        "\n",
        "    da_acc_seg = AverageMeter()\n",
        "    da_IoU_seg = AverageMeter()\n",
        "    da_mIoU_seg = AverageMeter()\n",
        "\n",
        "    ll_acc_seg = AverageMeter()\n",
        "    ll_IoU_seg = AverageMeter()\n",
        "    ll_mIoU_seg = AverageMeter()\n",
        "    total_batches = len(val_loader)\n",
        "\n",
        "    total_batches = len(val_loader)\n",
        "    pbar = enumerate(val_loader)\n",
        "    pbar = tqdm(pbar, total=total_batches)\n",
        "    for i, (_, _,input, target) in pbar:\n",
        "        input = input.cuda().float() / 255.0\n",
        "            # target = target.cuda()\n",
        "\n",
        "        input_var = input\n",
        "        target_var = target\n",
        "\n",
        "        # run the mdoel\n",
        "        with torch.no_grad():\n",
        "            output = model(input_var)\n",
        "\n",
        "        out_da,out_ll=output\n",
        "        target_da,target_ll=target\n",
        "\n",
        "        _,da_predict=torch.max(out_da, 1)\n",
        "        _,da_gt=torch.max(target_da, 1)\n",
        "\n",
        "        _,ll_predict=torch.max(out_ll, 1)\n",
        "        _,ll_gt=torch.max(target_ll, 1)\n",
        "        DA.reset()\n",
        "        DA.addBatch(da_predict.cpu(), da_gt.cpu())\n",
        "\n",
        "\n",
        "        da_acc = DA.pixelAccuracy()\n",
        "        da_IoU = DA.IntersectionOverUnion()\n",
        "        da_mIoU = DA.meanIntersectionOverUnion()\n",
        "\n",
        "        da_acc_seg.update(da_acc,input.size(0))\n",
        "        da_IoU_seg.update(da_IoU,input.size(0))\n",
        "        da_mIoU_seg.update(da_mIoU,input.size(0))\n",
        "\n",
        "\n",
        "        LL.reset()\n",
        "        LL.addBatch(ll_predict.cpu(), ll_gt.cpu())\n",
        "\n",
        "\n",
        "        ll_acc = LL.pixelAccuracy()\n",
        "        ll_IoU = LL.IntersectionOverUnion()\n",
        "        ll_mIoU = LL.meanIntersectionOverUnion()\n",
        "\n",
        "        ll_acc_seg.update(ll_acc,input.size(0))\n",
        "        ll_IoU_seg.update(ll_IoU,input.size(0))\n",
        "        ll_mIoU_seg.update(ll_mIoU,input.size(0))\n",
        "\n",
        "    da_segment_result = (da_acc_seg.avg,da_IoU_seg.avg,da_mIoU_seg.avg)\n",
        "    ll_segment_result = (ll_acc_seg.avg,ll_IoU_seg.avg,ll_mIoU_seg.avg)\n",
        "    return da_segment_result,ll_segment_result"
      ],
      "metadata": {
        "id": "_P6UMIX6hlef"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in list(range(1000)):\n",
        "    model.train()\n",
        "    model.to(\"cuda\")\n",
        "    output = model(input.cuda().float() / 255.0)\n",
        "\n",
        "    # target=target.cuda()\n",
        "    # print(target[0].size())\n",
        "    optimizer.zero_grad()\n",
        "    focal_loss, tversky_loss, loss = criterion(output,target)\n",
        "    optimizer.zero_grad()\n",
        "    # print(output.size())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % 50 == 0:\n",
        "        print(\"loss: {loss:.5f}\".format(loss = loss.item()))\n",
        "        model.eval()\n",
        "        example = torch.rand(1, 3, 360, 640).cuda()\n",
        "        model = torch.jit.trace(model, example)\n",
        "        print(\"Accuracy:\")\n",
        "        da_segment_results,ll_segment_results = val(train_dataloader, model)\n",
        "\n",
        "        msg =  'Driving area Segment: Acc({da_seg_acc:.3f})\\n' \\\n",
        "                            'Lane line Segment: Acc({ll_seg_acc:.3f})'.format(\n",
        "                                da_seg_acc=da_segment_results[0],\n",
        "                                ll_seg_acc=ll_segment_results[0])\n",
        "        print(msg)\n",
        "        print(\"Validation Evaluation\")\n",
        "        da_segment_results,ll_segment_results = val(val_dataloader, model)\n",
        "\n",
        "        msg =  'Driving area Segment: Acc({da_seg_acc:.3f})    IOU ({da_seg_iou:.3f})    mIOU({da_seg_miou:.3f})\\n' \\\n",
        "                            'Lane line Segment: Acc({ll_seg_acc:.3f})    IOU ({ll_seg_iou:.3f})  mIOU({ll_seg_miou:.3f})'.format(\n",
        "                                da_seg_acc=da_segment_results[0],da_seg_iou=da_segment_results[1],da_seg_miou=da_segment_results[2],\n",
        "                                ll_seg_acc=ll_segment_results[0],ll_seg_iou=ll_segment_results[1],ll_seg_miou=ll_segment_results[2])\n",
        "        print(msg)"
      ],
      "metadata": {
        "id": "9tWePJj0Zvvo",
        "outputId": "b2432b55-0f17-4c32-b13e-61e9447cedea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.34213\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.152)\n",
            "Lane line Segment: Acc(0.993)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:03<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.202)    IOU (0.202)    mIOU(0.101)\n",
            "Lane line Segment: Acc(0.981)    IOU (0.000)  mIOU(0.491)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py:787: UserWarning: The input to trace is already a ScriptModule, tracing it is a no-op. Returning the object as is.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.92347\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  8.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.872)\n",
            "Lane line Segment: Acc(0.994)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.797)    IOU (0.000)    mIOU(0.398)\n",
            "Lane line Segment: Acc(0.981)    IOU (0.000)  mIOU(0.491)\n",
            "loss: 0.83888\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  6.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.856)\n",
            "Lane line Segment: Acc(0.993)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  1.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.798)    IOU (0.000)    mIOU(0.399)\n",
            "Lane line Segment: Acc(0.981)    IOU (0.000)  mIOU(0.491)\n",
            "loss: 0.78081\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.855)\n",
            "Lane line Segment: Acc(0.993)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.796)    IOU (0.000)    mIOU(0.398)\n",
            "Lane line Segment: Acc(0.981)    IOU (0.000)  mIOU(0.491)\n",
            "loss: 0.57532\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.941)\n",
            "Lane line Segment: Acc(0.993)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.822)    IOU (0.225)    mIOU(0.518)\n",
            "Lane line Segment: Acc(0.981)    IOU (0.000)  mIOU(0.491)\n",
            "loss: 0.46402\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  9.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.934)\n",
            "Lane line Segment: Acc(0.994)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.824)    IOU (0.228)    mIOU(0.521)\n",
            "Lane line Segment: Acc(0.981)    IOU (0.000)  mIOU(0.491)\n",
            "loss: 0.31713\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  9.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.954)\n",
            "Lane line Segment: Acc(0.993)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.812)    IOU (0.224)    mIOU(0.513)\n",
            "Lane line Segment: Acc(0.979)    IOU (0.009)  mIOU(0.494)\n",
            "loss: 0.10646\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  5.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.945)\n",
            "Lane line Segment: Acc(0.995)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.817)    IOU (0.210)    mIOU(0.509)\n",
            "Lane line Segment: Acc(0.974)    IOU (0.013)  mIOU(0.493)\n",
            "loss: 0.09074\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.968)\n",
            "Lane line Segment: Acc(0.994)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.821)    IOU (0.215)    mIOU(0.514)\n",
            "Lane line Segment: Acc(0.975)    IOU (0.016)  mIOU(0.495)\n",
            "loss: 0.07511\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  9.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.943)\n",
            "Lane line Segment: Acc(0.993)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.829)    IOU (0.238)    mIOU(0.529)\n",
            "Lane line Segment: Acc(0.978)    IOU (0.017)  mIOU(0.498)\n",
            "loss: 0.06456\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  9.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.945)\n",
            "Lane line Segment: Acc(0.993)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.831)    IOU (0.240)    mIOU(0.531)\n",
            "Lane line Segment: Acc(0.979)    IOU (0.010)  mIOU(0.494)\n",
            "loss: 0.05576\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.926)\n",
            "Lane line Segment: Acc(0.995)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  1.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.830)    IOU (0.236)    mIOU(0.528)\n",
            "Lane line Segment: Acc(0.980)    IOU (0.010)  mIOU(0.495)\n",
            "loss: 0.04852\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  9.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.954)\n",
            "Lane line Segment: Acc(0.996)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.831)    IOU (0.278)    mIOU(0.549)\n",
            "Lane line Segment: Acc(0.981)    IOU (0.008)  mIOU(0.494)\n",
            "loss: 0.04271\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  9.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.910)\n",
            "Lane line Segment: Acc(0.992)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.830)    IOU (0.273)    mIOU(0.545)\n",
            "Lane line Segment: Acc(0.980)    IOU (0.007)  mIOU(0.494)\n",
            "loss: 0.03860\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.906)\n",
            "Lane line Segment: Acc(0.993)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.828)    IOU (0.282)    mIOU(0.548)\n",
            "Lane line Segment: Acc(0.979)    IOU (0.007)  mIOU(0.493)\n",
            "loss: 0.03481\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  9.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.938)\n",
            "Lane line Segment: Acc(0.995)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.826)    IOU (0.277)    mIOU(0.545)\n",
            "Lane line Segment: Acc(0.977)    IOU (0.007)  mIOU(0.492)\n",
            "loss: 0.03134\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  6.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.915)\n",
            "Lane line Segment: Acc(0.993)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.826)    IOU (0.287)    mIOU(0.550)\n",
            "Lane line Segment: Acc(0.975)    IOU (0.009)  mIOU(0.492)\n",
            "loss: 0.02738\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  8.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.955)\n",
            "Lane line Segment: Acc(0.997)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.827)    IOU (0.296)    mIOU(0.555)\n",
            "Lane line Segment: Acc(0.974)    IOU (0.017)  mIOU(0.495)\n",
            "loss: 0.02347\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.945)\n",
            "Lane line Segment: Acc(0.996)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.828)    IOU (0.286)    mIOU(0.551)\n",
            "Lane line Segment: Acc(0.974)    IOU (0.014)  mIOU(0.494)\n",
            "loss: 0.02074\n",
            "Accuracy:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  9.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.934)\n",
            "Lane line Segment: Acc(0.994)\n",
            "Validation Evaluation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driving area Segment: Acc(0.831)    IOU (0.266)    mIOU(0.542)\n",
            "Lane line Segment: Acc(0.973)    IOU (0.011)  mIOU(0.492)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOTBj528TU3TSWS9Se+ujbn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}